# Back-end/core/ai_models/llm_handler.py

import logging
import os
import json
from typing import List, Dict, Any, Optional
from groq import AsyncGroq
from core.config import settings
from core.ai_models.key_manager import groq_key_manager, gemini_key_manager

# üîë ‡πÑ‡∏°‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á client ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏° key ‡∏ó‡∏µ‡πà‡∏´‡∏°‡∏∏‡∏ô
def _get_groq_client() -> AsyncGroq:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Groq client ‡∏î‡πâ‡∏ß‡∏¢ key ‡∏ó‡∏µ‡πà‡∏´‡∏°‡∏∏‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
    api_key = groq_key_manager.get_key()
    if not api_key:
        raise RuntimeError("No Groq API keys available")
    masked = api_key[:8] + "..." + api_key[-4:]
    logging.info(f"üîë [Groq] Using key: {masked}")
    return AsyncGroq(api_key=api_key)

MAX_RETRIES = 4  # ‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô keys

async def get_llm_response(
    messages: List[Dict[str, str]], 
    model_name: str = settings.GROQ_LLAMA_MODEL,
    temperature: float = 0.3,
    max_tokens: int = 1024,
    json_mode: bool = False
) -> str:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏û‡∏£‡πâ‡∏≠‡∏° Key Rotation
    ‡∏£‡∏±‡∏ö messages ‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á dict ‡πÄ‡∏ä‡πà‡∏ô:
    [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]
    """
    last_error = None
    
    for attempt in range(MAX_RETRIES):
        try:
            groq_client = _get_groq_client()
            
            kwargs = {
                "model": model_name,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
            if json_mode:
                kwargs["response_format"] = {"type": "json_object"}

            response = await groq_client.chat.completions.create(**kwargs)
            return response.choices[0].message.content
            
        except Exception as e:
            last_error = e
            error_str = str(e).lower()
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô rate limit error ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á key ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            rate_limit_keywords = ["rate", "429", "quota", "exceeded", "limit", "exhausted"]
            is_rate_limit = any(keyword in error_str for keyword in rate_limit_keywords)
            
            if is_rate_limit:
                logging.warning(f"‚ö†Ô∏è [Groq] Rate limit hit, rotating key... (attempt {attempt + 1}/{MAX_RETRIES})")
                continue
            else:
                # Error ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á retry
                logging.error(f"‚ùå [LLM Handler] Error calling Groq: {e}")
                break
    
    logging.error(f"‚ùå [LLM Handler] All retries failed: {last_error}")
    return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (LLM Error)"

async def get_llama_response_direct_async(user_query: str) -> str:
    """
    ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Small Talk / ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏°‡∏µ system prompt ‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏•‡∏±‡∏ö + Multi-language support
    """
    system_prompt = """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô ‡πÑ‡∏Å‡∏î‡πå‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô

üåê ‡∏Å‡∏é‡∏†‡∏≤‡∏©‡∏≤ (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!):
- ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‚Üí ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÉ‡∏ä‡πâ "‡∏Ñ‡πà‡∏∞"
- If user speaks English ‚Üí Reply in English naturally
- Â¶ÇÊûúÁî®Êà∑Áî®‰∏≠Êñá ‚Üí Áî®‰∏≠ÊñáÂõûÁ≠î

‡∏Å‡∏é‡∏≠‡∏∑‡πà‡∏ô‡πÜ:
1. ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö (2-3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ)
2. ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏™‡∏°‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
3. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ô‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô ‡πÉ‡∏´‡πâ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
- "‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏à‡∏µ‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö" ‚Üí "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏∞! ‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡πÑ‡∏ó‡∏•‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏ô‡∏∞‡∏Ñ‡∏∞ üéâ"
- "I'm from Japan" ‚Üí "Welcome! Nan has beautiful temples and culture you'll love üéâ"
- "ÊàëÊù•Ëá™‰∏≠ÂõΩ" ‚Üí "Ê¨¢ËøéÂÖâ‰∏¥ÔºÅÂçóÂ∫úÊúâÂæàÂ§öÁ≤æÂΩ©ÁöÑÊôØÁÇπÁ≠âÊÇ®Êé¢Á¥¢ üéâ"
"""
    
    return await get_llm_response(
        [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ],
        model_name=settings.GROQ_SMALL_TALK_MODEL,
        temperature=0.7 
    )

async def get_groq_rag_response_async(user_query: str, context: str, insights: str = "", turn_count: int = 1, ai_mode: str = "fast") -> Dict[str, Any]:
    """
    ai_mode: 'fast' = Groq/Llama, 'detailed' = Gemini
    ‡∏û‡∏£‡πâ‡∏≠‡∏° Multi-language support
    """
    # üåê Multi-language instruction
    language_rule = """üåê ‡∏Å‡∏é‡∏†‡∏≤‡∏©‡∏≤: ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏°
- ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‚Üí ‡∏ï‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÉ‡∏ä‡πâ "‡∏Ñ‡πà‡∏∞"
- English ‚Üí Reply in English
- ‰∏≠Êñá ‚Üí Áî®‰∏≠ÊñáÂõûÁ≠î"""
    
    system_msg = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô ‡πÑ‡∏Å‡∏î‡πå‡∏ô‡∏≥‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô

{language_rule}

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:
{context}"""
    
    if insights:
        system_msg += f"\n\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:\n{insights}"

    if ai_mode == "detailed":
        # Use Gemini for detailed responses
        response_text = await get_gemini_response_async(
            user_query=user_query,
            system_prompt=system_msg,
            max_tokens=2048  # Allow longer responses
        )
    else:
        # Use Groq/Llama for fast responses
        response_text = await get_llm_response([
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_query}
        ])
    
    return {"answer": response_text, "sources_used": []}

# ========================================
# Gemini Support for Detailed Mode
# ========================================
import google.generativeai as genai

# üîë ‡πÑ‡∏°‡πà configure ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ - configure ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏° key ‡∏ó‡∏µ‡πà‡∏´‡∏°‡∏∏‡∏ô

async def get_gemini_response_async(
    user_query: str,
    system_prompt: str = "",
    model_name: str = "gemini-2.5-flash",
    max_tokens: int = 2048
) -> str:
    """
    ‡πÉ‡∏ä‡πâ Gemini ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detailed mode - ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏¢‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏ß‡πà‡∏≤
    ‡∏û‡∏£‡πâ‡∏≠‡∏° Key Rotation ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠ Rate Limit
    """
    import asyncio
    last_error = None
    
    for attempt in range(MAX_RETRIES):
        try:
            # üîë ‡∏´‡∏°‡∏∏‡∏ô key ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å
            api_key = gemini_key_manager.get_key()
            if not api_key:
                raise RuntimeError("No Gemini API keys available")
            
            masked = api_key[:8] + "..." + api_key[-4:]
            logging.info(f"üîë [Gemini] Using key: {masked}")
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model_name)
            
            full_prompt = f"{system_prompt}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {user_query}\n\n‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô:"
            
            # Run in thread pool since google-generativeai is synchronous
            response = await asyncio.to_thread(
                model.generate_content,
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=0.7
                )
            )
            
            return response.text
            
        except Exception as e:
            last_error = e
            error_str = str(e).lower()
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô rate limit/quota error ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á key ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà Google API ‡∏≠‡∏≤‡∏à‡∏™‡πà‡∏á‡∏°‡∏≤
            rate_limit_keywords = ["rate", "429", "quota", "resource", "exceeded", "limit", "exhausted"]
            is_rate_limit = any(keyword in error_str for keyword in rate_limit_keywords)
            
            if is_rate_limit:
                logging.warning(f"‚ö†Ô∏è [Gemini] Rate limit hit, rotating key... (attempt {attempt + 1}/{MAX_RETRIES})")
                continue
            else:
                # Error ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á retry
                logging.error(f"‚ùå [Gemini] Error: {e}")
                break
    
    logging.error(f"‚ùå [Gemini] All retries failed: {last_error}")
    return f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (Gemini Error: {str(last_error)[:100]})"